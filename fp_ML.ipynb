{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import enlighten\n",
    "from collections import defaultdict as ddict\n",
    "\n",
    "#!source activate my-rdkit-env && python\n",
    "from rdkit import RDLogger\n",
    "from rdkit.Chem import PandasTools, AllChem as Chem, Descriptors\n",
    "from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator\n",
    "from rdkit.Chem.rdmolops import SanitizeFlags\n",
    "\n",
    "# from rdkit import Chem\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "#ML requirements sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "RDLogger.logger().setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "# RDLogger.logger().setLevel(RDLogger.WARNING)\n",
    "\n",
    "not_used_desc = ['MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge']\n",
    "#molecule descriptor calculator\n",
    "desc_calc = MolecularDescriptorCalculator([x for x in [x[0] for x in Descriptors.descList] if x not in not_used_desc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write some stuff\n",
    "def stats(A,B):\n",
    "    assert len(A) == len(B)\n",
    "    R2 = np.round(r2_score(A, B))\n",
    "    MAE = np.round(mean_absolute_error(A,B))\n",
    "    return f'R2: {np.round(R2, 3)}\\n' \\\n",
    "            f'MAE: {np.round(MAE, 3)}'\n",
    "\n",
    "def show(df):\n",
    "    \"\"\"Render the molecules within a DataFrame correctly\"\"\"\n",
    "    return HTML(df.to_html(notebook=True))\n",
    "\n",
    "def remove_aromatisation(mol):\n",
    "    # for atom in mol.GetAromaticAtoms():\n",
    "    #     #bugfixing\n",
    "    #     # print(atom.getIsAromatic())\n",
    "    #     # print(atom.getSymbol())\n",
    "    #     atom.SetIsAromatic(False)\n",
    "\n",
    "    for atom in mol.GetAtoms():\n",
    "        if (not atom.IsInRing()) and atom.GetIsAromatic():\n",
    "            atom.SetIsAromatic(False)\n",
    "    for bond in mol.GetBonds():\n",
    "        if (not bond.IsInRing()) and bond.GetIsAromatic():\n",
    "            bond.SetIsAromatic(False)\n",
    "\n",
    "def get_frag_env(mol, atom_x, radius):\n",
    "    bond_x = Chem.FindAtomEnvironmentOfRadiusN(mol, radius, atom_x)\n",
    "    if len(bond_x) == 0:\n",
    "        return None\n",
    "    atom_x_set = set()\n",
    "    for b_x in bond_x:\n",
    "        b = mol.GetBondWithIdx(b_x)\n",
    "        atom_x_set.add(b.GetBeginAtomIdx())\n",
    "        atom_x_set.add(b.GetEndAtomIdx())\n",
    "    return Chem.MolFromSmiles(Chem.MolFragmentToSmiles(mol, atom_x_set, bond_x), sanitize=False)\n",
    "\n",
    "def get_frag(mol, atom_x, radius):\n",
    "    frag = None\n",
    "    r = radius\n",
    "\n",
    "    while frag is None and r > 0:\n",
    "        frag = get_frag_env(mol, atom_x, r)\n",
    "        r -= 1\n",
    "    if frag is None:\n",
    "        raise ValueError(\"No fragment extracted\")\n",
    "\n",
    "    # error/issue catching#or\n",
    "    r = radius + 1\n",
    "    #sanitised to calculated descriptors\n",
    "    not_sanitised = Chem.SanitizeMol(frag, catchErrors=True)\n",
    "    count = 0\n",
    "    tot_atoms = frag.GetNumAtoms()\n",
    "    while not_sanitised:\n",
    "        if count > tot_atoms:\n",
    "            raise ValueError(\"Unable to sanitise molecules\")\n",
    "        # probs = Chem.DetectChemistryProblems(frag)\n",
    "        # for p in probs:\n",
    "        #     print(\"Issue is: \" + p.GetType())\n",
    "        if not_sanitised == SanitizeFlags.SANITIZE_KEKULIZE:\n",
    "            # if count == 5:\n",
    "            #     raise ValueError(\"Unable to sanitise molecules\")\n",
    "            remove_aromatisation(frag)\n",
    "        elif not_sanitised == SanitizeFlags.SANITIZE_PROPERTIES:\n",
    "            frag = get_frag_env(mol, atom_x, r)\n",
    "            r += 1\n",
    "        not_sanitised = Chem.SanitizeMol(frag, catchErrors=True)\n",
    "        count += 1\n",
    "    # print(Chem.MolToSmiles(frag))\n",
    "    return frag\n",
    "\n",
    "# def fix_nitrogen(mol):\n",
    "#     mol.UpdatePropertyCache(strict=False)\n",
    "#     ps = Chem.DetectChemistryProblems(mol)\n",
    "#     if ps:\n",
    "#         for p in ps:\n",
    "#             if p.GetType() == 'AtomValenceException':\n",
    "#                 at = mol.GetAtomWithIdx(p.GetAtomIdx())\n",
    "#             if at.GetAtomicNum()==7 and at.GetFormalCharge()==0 and at.GetExplicitValence()==4:\n",
    "#                 at.SetFormalCharge(1)\n",
    "#     if not ps:\n",
    "#         print(\"Chemically fine\")\n",
    "    # return mol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assay identification number\n",
    "#AID as written in PubChem database\n",
    "\n",
    "#hERG ion channel modulator assay\n",
    "assay = 1511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup excel method\n",
    "excelfile = \"AID_1511_datatable_all.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile Assay molecules into SDF\n",
    "from pubapi import get_data_AID_csv as assayinfo\n",
    "\n",
    "#get assay info\n",
    "\n",
    "try:\n",
    "    assay_info = assayinfo.get_data_AID_csv(assay)\n",
    "\n",
    "except:\n",
    "    print(\"Error in download\")\n",
    "    ERROR_FLAG = True\n",
    "\n",
    "\n",
    "\n",
    "# assay_info = assayinfo.get_data_AID_csv(assay)\n",
    "\n",
    "# with open(excelfile, 'r') as csvfile:\n",
    "#     reader = csv.reader(csvfile, delimiter=',')\n",
    "#     assay_info = []\n",
    "#     headers = ['PUBCHEM_ASSAY_ID']\n",
    "#     for id, row in enumerate(reader):\n",
    "#         if id == 0:\n",
    "#             headers.append(row[1:4])\n",
    "#             assay_info.append(headers)\n",
    "#         try: \n",
    "#             int(row[0]) == 1\n",
    "#         except ValueError:\n",
    "#             #line is descriptive\n",
    "#             continue\n",
    "#         if row[2] == '':\n",
    "#             #CID is missing/not present\n",
    "#             row[2] = 0\n",
    "#             #continue\n",
    "\n",
    "#         line = [assay, int(row[1]), int(row[2]), row[3]]\n",
    "#         assay_info.append(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame(assay_info[1:],columns=assay_info[0])\n",
    "\n",
    "# all_df = pd.DataFrame(assay_info[1:],columns=assay_info[0])\n",
    "#get list of CIDs from the assay\n",
    "print(all_df.head(10))\n",
    "\n",
    "CIDs = all_df['PUBCHEM_CID'].to_list()\n",
    "\n",
    "# from pubapi import get_SDF_CIDs as SDFs\n",
    "\n",
    "# SDFs.compile_SDFs(CIDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"PUBCHEM_CANONICAL_SMILES\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set error flag if desired\n",
    "ERROR_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the SMILES from the SQL database to each of the CIDs in the \n",
    "\n",
    "#do it with internal mariadb\n",
    "#uncomment\n",
    "#↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "\n",
    "from search_laurus import query_db\n",
    "\n",
    "query = f'SELECT * FROM aggregator.pubchem_compound WHERE ' \n",
    "\n",
    "pbar = enlighten.Counter(total=(len(CIDs)), desc='Querying Laurus...', unit='ticks')\n",
    "\n",
    "if not ERROR_FLAG:\n",
    "    try:\n",
    "        for ID, i in enumerate(CIDs):\n",
    "            # if ID == 25000:\n",
    "            #     break\n",
    "            #Search for CIDs\n",
    "            CID_search = query + f'pubchem_id = {i};' \n",
    "            info = query_db(CID_search)\n",
    "\n",
    "            #parse\n",
    "            for j in info:\n",
    "                # pubchem = j[0]\n",
    "                # compound = j[1]\n",
    "                smiles = j[5]\n",
    "                all_df.loc[all_df[\"PUBCHEM_CID\"] == i, 'PUBCHEM_CANONICAL_SMILES'] = smiles\n",
    "\n",
    "            pbar.update()\n",
    "    except:\n",
    "        print(\"Error in connecting to Laurus database\")\n",
    "        ERROR_FLAG = True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup method to load dataframe\n",
    "if not ERROR_FLAG:\n",
    "    backup = all_df.copy()\n",
    "    all_df.to_csv('backup_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df upon laurus/pubchem failure\n",
    "if ERROR_FLAG:\n",
    "    all_df = pd.read_csv('backup_df.csv')\n",
    "    ERROR_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now convert the SMILES strings to MOL objects\n",
    "if not ERROR_FLAG:\n",
    "    PandasTools.AddMoleculeColumnToFrame(all_df, 'PUBCHEM_CANONICAL_SMILES', 'ROMol', includeFingerprints=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write molecules to SDF\n",
    "if not ERROR_FLAG:\n",
    "    PandasTools.WriteSDF(all_df, f'{assay}_compounds.sdf', molColName='ROMol', properties=all_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load SDF file\n",
    "\n",
    "if ERROR_FLAG:\n",
    "    print(\"Error in downloading data, using backup data (if available)\")\n",
    "    import os\n",
    "\n",
    "    sdf_path = os.path.join(os.getcwd(), f'{assay}_compounds.sdf')\n",
    "    backup_df = PandasTools.LoadSDF(sdf_path)\n",
    "    training_data = backup_df.copy()\n",
    "    \n",
    "else:\n",
    "    training_data = all_df.copy()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(training_data.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#get fingerprints and split into fragments.\n",
    "min_radius = 3\n",
    "max_radius = 5\n",
    "\n",
    "radii = list(range(min_radius, max_radius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frags = ddict(list)\n",
    "\n",
    "# #check to see how long this may actually take\n",
    "# frag_prog = enlighten.Counter(total=(len(test_df)), desc='Creating Fragments...', unit='ticks')\n",
    "\n",
    "# # show(test_df.drop(129607, axis=0))\n",
    "\n",
    "# for index, row in test_df.iterrows():\n",
    "#     atom_x = 0\n",
    "#     mol = row.MOL\n",
    "#     check_frags = []\n",
    "#     for i in radii:\n",
    "#         try:\n",
    "#             # frags[i].append(get_frag(mol, atom_x, i))\n",
    "#             frag = get_frag(mol, atom_x, i)\n",
    "#             check_frags.append((i, frag))\n",
    "#         except:\n",
    "#             print(index)\n",
    "#             # test_df.drop(index, axis=0, inplace=True)\n",
    "\n",
    "#     if len(check_frags) != len(radii):\n",
    "#         test_df.drop(index, axis=0, inplace=True)\n",
    "#     else:\n",
    "#         for i in check_frags:\n",
    "#             frags[i[0]].append(i[1])\n",
    "#     frag_prog.update()\n",
    "\n",
    "# for i in radii:\n",
    "#     test_df[f'FRAG_R{i}'] = frags[i]\n",
    "\n",
    "# show(test_df.head(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frags = ddict(list)\n",
    "\n",
    "#check to see how long this may actually take\n",
    "frag_prog = enlighten.Counter(total=(len(training_data)), desc='Creating Fragments...', unit='ticks')\n",
    "\n",
    "for index, row in training_data.iterrows():\n",
    "    atom_x = 0\n",
    "    mol = row.ROMol\n",
    "    check_frags = []\n",
    "    for i in radii:\n",
    "        try:\n",
    "            # frags[i].append(get_frag(mol, atom_x, i))\n",
    "            frag = get_frag(mol, atom_x, i)\n",
    "            check_frags.append((i, frag))\n",
    "        except:\n",
    "            print(f'Removing line: {index}')\n",
    "    if len(check_frags) != len(radii):\n",
    "        training_data.drop(index, axis=0, inplace=True)\n",
    "    else:\n",
    "        for i in check_frags:\n",
    "            frags[i[0]].append(i[1])\n",
    "            \n",
    "    frag_prog.update()\n",
    "\n",
    "\n",
    "for i in radii:\n",
    "    training_data[f'FRAG_R{i}'] = frags[i]\n",
    "\n",
    "# show(training_data.head(4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(training_data.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = radii + [max_radius]\n",
    "# descs_check = ddict(list)\n",
    "\n",
    "# limit = 2\n",
    "\n",
    "# for index, row in training_data.iterrows():\n",
    "#     if index > limit:\n",
    "#         break\n",
    "#     for i in keys:\n",
    "#         mol = row.MOL if i == max_radius else row[f'FRAG_R{i}']\n",
    "#         descs_check[i].append(desc_calc.CalcDescriptors(mol))\n",
    "\n",
    "# print(len(descs_check[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate descriptors for fragments\n",
    "\n",
    "keys = radii + [max_radius]\n",
    "\n",
    "#this guy used a dictionary like a list...\n",
    "descr = ddict(list)\n",
    "\n",
    "morgan0 = ddict(list)\n",
    "morgan1 = ddict(list)\n",
    "morgan2 = ddict(list)\n",
    "morgan3 = ddict(list)\n",
    "topol = ddict(list)\n",
    "atpair = ddict(list)\n",
    "\n",
    "\n",
    "descriptor_pb = enlighten.Counter(total=(len(training_data)), desc='Calculating descriptors...', unit='ticks')\n",
    "\n",
    "# limit = 2\n",
    "\n",
    "for index, row in training_data.iterrows():\n",
    "    nan_flag = False\n",
    "\n",
    "    #list indexing > dictionaries\n",
    "    descriptors_check = []\n",
    "    ecpf_check = []\n",
    "    # if index > limit:\n",
    "    #     break\n",
    "    for i in keys:\n",
    "        # print(i)\n",
    "        mol = row.ROMol if i == max_radius else row[f'FRAG_R{i}']   \n",
    "        descriptor = desc_calc.CalcDescriptors(mol)\n",
    "        for j in descriptor:\n",
    "            if np.isnan(j):\n",
    "                nan_flag = True\n",
    "                break \n",
    "        if nan_flag:\n",
    "            print(index)\n",
    "            break\n",
    "        descriptors_check.append(descriptor)\n",
    "\n",
    "        #ECPF    \n",
    "        morgan_0vec = Chem.GetMorganFingerprintAsBitVect(mol, radius=0)\n",
    "        morgan_1vec = Chem.GetMorganFingerprintAsBitVect(mol, radius=1)\n",
    "        morgan_2vec = Chem.GetMorganFingerprintAsBitVect(mol, radius=2)\n",
    "        morgan_3vec = Chem.GetMorganFingerprintAsBitVect(mol, radius=3)\n",
    "\n",
    "        topol_vec =  Chem.GetHashedTopologicalTorsionFingerprintAsBitVect(mol)\n",
    "\n",
    "        atpair_vec = Chem.GetHashedAtomPairFingerprintAsBitVect(mol)\n",
    "\n",
    "        check_vecs = [morgan_0vec, morgan_1vec, morgan_2vec, morgan_3vec, topol_vec, atpair_vec]\n",
    "        for k in check_vecs:\n",
    "            array = np.array(i)\n",
    "            nan_flan = np.isnan(array).any()\n",
    "        if nan_flag:\n",
    "            break\n",
    "\n",
    "        ecpf_check.append(check_vecs)\n",
    "\n",
    "        # descr[i].append(descriptor)\n",
    "        #issue is it may append before molecules are checked\n",
    "        \n",
    "        \n",
    "    if nan_flag:\n",
    "        print(f'Line {index} contains NaN values, removing...')\n",
    "        training_data.drop(index, inplace=True)\n",
    "    else:\n",
    "        for ID, l in enumerate(keys):\n",
    "            descr[l].append(descriptors_check[ID])\n",
    "\n",
    "        for m in ecpf_check:\n",
    "            for ID, n in enumerate(keys):\n",
    "                morgan0[n].append(k[ID])\n",
    "                morgan1[n].append(k[ID])\n",
    "                morgan2[n].append(k[ID])\n",
    "                morgan3[n].append(k[ID])\n",
    "                topol[n].append(k[ID])\n",
    "                atpair[n].append(k[ID])\n",
    "        \n",
    "\n",
    "    #testing the length of the frag dict\n",
    "    descriptor_pb.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create fingerprints for all molecules/fragments\n",
    "\n",
    "# morgan0 = ddict(list)\n",
    "# morgan1 = ddict(list)\n",
    "# morgan2 = ddict(list)\n",
    "# morgan3 = ddict(list)\n",
    "# topol = ddict(list)\n",
    "# atpair = ddict(list)\n",
    "\n",
    "# fingerprint_pb = enlighten.Counter(total=(len(training_data)), desc='Calculating fingerprints...', unit='ticks')\n",
    "\n",
    "# for index, row in training_data.iterrows():\n",
    "#     nan_flag = False\n",
    "#     descriptors_check = []\n",
    "#     for i in keys:\n",
    "#         mol = row.ROMol if i == max_radius else row[f'FRAG_R{i}']\n",
    "\n",
    "#         morgan_0vec = Chem.GetMorganFingerprintAsBitVect(mol, radius=0)\n",
    "#         morgan_1vec = Chem.GetMorganFingerprintAsBitVect(mol, radius=1)\n",
    "#         morgan_2vec = Chem.GetMorganFingerprintAsBitVect(mol, radius=2)\n",
    "#         morgan_3vec = Chem.GetMorganFingerprintAsBitVect(mol, radius=3)\n",
    "\n",
    "#         topol_vec =  Chem.GetHashedTopologicalTorsionFingerprintAsBitVect(mol)\n",
    "\n",
    "#         atpair_vec = Chem.GetHashedAtomPairFingerprintAsBitVect(mol)\n",
    "\n",
    "#         check_vecs = [morgan_0vec, morgan_1vec, morgan_2vec, morgan_3vec, topol_vec, atpair_vec]\n",
    "#         for j in check_vecs:\n",
    "#             array = np.array(i)\n",
    "#             nan_flan = np.isnan(array).any()\n",
    "#         if nan_flag:\n",
    "#             break\n",
    "\n",
    "#         descriptors_check.append(check_vecs)\n",
    "\n",
    "#         # morgan0[i].append(Chem.GetMorganFingerprintAsBitVect(mol, radius=0))\n",
    "#         # morgan1[i].append(Chem.GetMorganFingerprintAsBitVect(mol, radius=1))\n",
    "#         # morgan2[i].append(Chem.GetMorganFingerprintAsBitVect(mol, radius=2))\n",
    "#         # morgan3[i].append(Chem.GetMorganFingerprintAsBitVect(mol, radius=3))\n",
    "#         # topol[i].append(Chem.GetHashedTopologicalTorsionFingerprintAsBitVect(mol))\n",
    "#         # atpair[i].append(Chem.GetHashedAtomPairFingerprintAsBitVect(mol))\n",
    "    \n",
    "#     if nan_flag:\n",
    "#         print(f'Line {index} contains NaN values, removing...')\n",
    "#         training_data.drop(index, inplace=True)\n",
    "#     else:\n",
    "#         for k in descriptors_check:\n",
    "#             for ID, l in enumerate(keys):\n",
    "#                 morgan0[l].append(k[ID])\n",
    "#                 morgan1[l].append(k[ID])\n",
    "#                 morgan2[l].append(k[ID])\n",
    "#                 morgan3[l].append(k[ID])\n",
    "#                 topol[l].append(k[ID])\n",
    "#                 atpair[l].append(k[ID])\n",
    "\n",
    "\n",
    "#     fingerprint_pb.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #replace activity with 1 if active and 0 if inactive\n",
    "training_data[\"PUBCHEM_ACTIVITY_OUTCOME\"].replace(\"Inactive\", 0, inplace=True)\n",
    "training_data[\"PUBCHEM_ACTIVITY_OUTCOME\"].replace(\"Active\", 1, inplace=True)\n",
    "\n",
    "# show(training_data.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.iloc[training_data.values==np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data[\"PUBCHEM_ACTIVITY_OUTCOME\"].replace(0, \"Inactive\", inplace=True)\n",
    "# training_data[\"PUBCHEM_ACTIVITY_OUTCOME\"].replace(1, \"Active\", inplace=True)\n",
    "\n",
    "# show(training_data.head(4))\n",
    "\n",
    "\n",
    "# y_dataset_test = training_data[\"PUBCHEM_ACTIVITY_OUTCOME\"]\n",
    "# encode = LabelEncoder()\n",
    "# encode.fit(y_dataset_test)\n",
    "# encoded_y = encode.transform(y_dataset_test)\n",
    "\n",
    "# # print(encoded_y)\n",
    "# show(training_data.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 20\n",
    "\n",
    "folds = 5\n",
    "jobs = 12\n",
    "\n",
    "y_data = training_data[\"PUBCHEM_ACTIVITY_OUTCOME\"].to_list()\n",
    "\n",
    "# y_data = encoded_y\n",
    "x_data = dict(Descriptors=descr, Morgan0=morgan0, Morgan1=morgan1, Morgan2=morgan2, \n",
    "                  Morgan3=morgan3, Topological=topol, AtomPair=atpair)\n",
    "\n",
    "# kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "# spl = list(kf.split(descr[max_radius], y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dsetname, dset in x_data.items():\n",
    "#     # print(dset)\n",
    "#     print(f'Random Forest - {dsetname}:')\n",
    "#     scores = {}\n",
    "#     for i in dset:\n",
    "#         print(i)\n",
    "#         array = np.array(dset[i])\n",
    "#         nan = np.isnan(array).any()\n",
    "#         inf = np.isinf(array).any()\n",
    "#         print(nan)\n",
    "#         print(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe skip this step\n",
    "#issue is with the radius 5 descriptors\n",
    "#cross validation of the possible descriptors\n",
    "\n",
    "#still error with radius 5 molecules, getting NaN errors\n",
    "\n",
    "# all_scores = {}\n",
    "# for dsetname, dset in x_data.items():\n",
    "#     # print(dset)\n",
    "#     print(f'Random Forest - {dsetname}:')\n",
    "#     scores = {}\n",
    "#     for i in dset:\n",
    "#         print(i)\n",
    "#         print(type(i))\n",
    "#         if i < 4:\n",
    "#             continue\n",
    "#         print(dset[i])\n",
    "#         with open('checkthisbullshit.txt', 'w') as nanfile:\n",
    "#             nanfile.write('\\n'.join('{} {}'.format(x[0], x[1]) for x in dset[i]))\n",
    "#             # nanfile.writelines(dset[i])\n",
    "#             print(\"File written\")\n",
    "\n",
    "#         print(np.isnan(dset[i]).any())\n",
    "#         print(np.isinf(dset[i]).any())\n",
    "#         print(~np.isfinite(dset[i]).any())\n",
    "#         print(\"Checks done\")\n",
    "#         est = RandomForestClassifier(n_estimators=trees, random_state=seed, n_jobs=jobs)\n",
    "#         score = cross_val_score(est, dset[i], y_data, cv=spl, error_score='raise')\n",
    "#         print(score)\n",
    "#         scores[i] = dict(r2=np.mean(score), r2_std=np.std(score))\n",
    "#         print(f'\\tRadius {i if i != max_radius else \"M\"} - R²: '\n",
    "#               f'{np.round(scores[i][\"r2\"], 3)} ± {np.round(scores[i][\"r2_std\"], 3)}')\n",
    "#     all_scores[dsetname] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test split\n",
    "# jobs = 12\n",
    "radius = 5\n",
    "test_partition = 0.2\n",
    "#TODO: NaNs NEED TO BE REMOVED, FIND A METHOD TO DO THIS\n",
    "# print(len(descr[radius]))\n",
    "# print(len(y_data))\n",
    "\n",
    "assert len(descr[radius]) == len(y_data)\n",
    "x_dataset = np.array(descr[radius], dtype=np.float64)\n",
    "y_data = np.array(y_data, dtype=np.float64)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(descr[radius], y_data, test_size=test_partition, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final check for NaNs\n",
    "err =np.argwhere(np.isnan(x_train))\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err =np.argwhere(np.isnan(y_train))\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[27723])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fire away\n",
    "rf = RandomForestClassifier(n_estimators=trees, random_state=seed, n_jobs=jobs)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err2 =np.argwhere(np.isinf(x_test))\n",
    "print(err2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('check.txt', 'w') as ifile:\n",
    "    for i in x_test:\n",
    "        ifile.write(' '.join(str(s) for s in i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rf.predict(x_test)\n",
    "print(stats(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance\n",
    "feat_import = pd.DataFrame(zip(desc_calc.descriptorNames, rf.feature_importances_),\n",
    "                            columns=['Descriptor', 'Importance']).sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_import.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_scaler = StandardScaler()\n",
    "svr_scaler"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
